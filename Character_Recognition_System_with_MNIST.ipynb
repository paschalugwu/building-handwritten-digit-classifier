{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNsLLEHsFKiQQFDlZGriCrL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Project: Train a Character Recognition System with MNIST"],"metadata":{"id":"pjaRgvjvLtSa"}},{"cell_type":"markdown","source":["## Section 1: Data Loading and Exploration"],"metadata":{"id":"hTP8_shUL3p4"}},{"cell_type":"markdown","source":["### Step 1: Load the Dataset\n","\n","In this step, we will load the MNIST dataset using `torchvision.datasets`. This dataset consists of 60,000 training images and 10,000 test images of handwritten digits (0-9). We will preprocess the data by converting it to tensors and normalizing it. We'll also flatten the images to one-dimensional vectors as required for input into a fully connected neural network."],"metadata":{"id":"IR9qmjHmMgIc"}},{"cell_type":"code","source":["# Import torch and torchvision libraries\n","import torch\n","import torchvision\n","# Import transforms from torchvision\n","from torchvision import transforms\n","\n","# Define the transformations\n","transform = transforms.Compose([\n","    transforms.ToTensor(),  # Convert the images to tensors\n","    transforms.Normalize((0.1307,), (0.3081,)),  # Normalize the images with MNIST mean and std\n","    transforms.Lambda(lambda x: x.view(-1))  # Flatten the images into one-dimensional vectors\n","])\n","\n","# Load the training and test datasets from torchvision.datasets\n","train_dataset = torchvision.datasets.MNIST(\n","    root='./data', train=True, transform=transform, download=True)\n","test_dataset = torchvision.datasets.MNIST(\n","    root='./data', train=False, transform=transform, download=True)\n","\n","# Define batch size\n","batch_size = 64\n","\n","# Create DataLoader objects for training and test datasets\n","train_loader = torch.utils.data.DataLoader(\n","    dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(\n","    dataset=test_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"IP5UYVRGMBGn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716457947685,"user_tz":-60,"elapsed":15816,"user":{"displayName":"Paschal Ugwu","userId":"02920440952136523881"}},"outputId":"26978099-9d62-43c2-95c3-73ee8438a508"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 11736196.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 523255.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:01<00:00, 924534.35it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 4862309.54it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["### Step 2: Visualize the Dataset\n","\n","In this step, we will visualize the dataset to understand the properties of the images. We will plot a few samples from the dataset before and after preprocessing. This helps in verifying that our transformations are correct and gives us an intuition about the data."],"metadata":{"id":"yHlUwI-sMG9K"}},{"cell_type":"code","source":["# Import matplotlib for plotting\n","import matplotlib.pyplot as plt\n","\n","def visualize_batch(images, labels, unnormalize=True):\n","    \"\"\"\n","    Visualizes a batch of images.\n","\n","    Args:\n","        images (torch.Tensor): Batch of images.\n","        labels (torch.Tensor): Corresponding labels.\n","        unnormalize (bool): Whether to unnormalize the images.\n","    \"\"\"\n","    # Unnormalize the images if necessary\n","    if unnormalize:\n","        images = images * 0.3081 + 0.1307\n","    images = images.view(images.size(0), 28, 28)  # Reshape to 28x28\n","\n","    # Plot the images in a grid\n","    fig, axes = plt.subplots(1, len(images), figsize=(12, 12))\n","    for img, lbl, ax in zip(images, labels, axes):\n","        ax.imshow(img, cmap='gray')\n","        ax.set_title(f'Label: {lbl.item()}')\n","        ax.axis('off')\n","    plt.show()\n","\n","# Load a batch of images from the training DataLoader\n","images, labels = next(iter(train_loader))\n","\n","# Call the visualization function on this batch\n","visualize_batch(images[:8], labels[:8])  # Visualize first 8 images\n","\n","# Display the shape and size of the datasets (both training and test)\n","print(f'Training dataset size: {len(train_dataset)}')\n","print(f'Test dataset size: {len(test_dataset)}')\n","\n","# Print the shape of a single image before and after flattening\n","sample_image, _ = train_dataset[0]  # Get a sample image\n","print(f'Original shape: {sample_image.view(28, 28).shape}')\n","print(f'Flattened shape: {sample_image.shape}')"],"metadata":{"id":"FxOnZgobMNVk","colab":{"base_uri":"https://localhost:8080/","height":230},"executionInfo":{"status":"ok","timestamp":1716457948823,"user_tz":-60,"elapsed":1145,"user":{"displayName":"Paschal Ugwu","userId":"02920440952136523881"}},"outputId":"dba03519-19cb-48df-8183-2f0f2a3855fe"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x1200 with 8 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Training dataset size: 60000\n","Test dataset size: 10000\n","Original shape: torch.Size([28, 28])\n","Flattened shape: torch.Size([784])\n"]}]},{"cell_type":"markdown","source":["### Justify the Preprocessing Steps\n","\n","#### Why Convert to Tensors?\n","\n","Converting the images to tensors is a crucial preprocessing step because PyTorch models expect input data to be in the form of tensors. Tensors are the core data structure used by PyTorch for all operations, and they provide powerful capabilities for numerical computation, which is essential for training deep learning models. Moreover, tensors support automatic differentiation, which is necessary for the backpropagation algorithm used in training neural networks.\n","\n","#### Why Normalize the Images?\n","\n","Normalization is performed to scale the pixel values of the images to a range that is more suitable for neural network training. The MNIST dataset has pixel values ranging from 0 to 255. Normalizing these values to a range of approximately -1 to 1 (using the mean and standard deviation specific to MNIST) helps in stabilizing and speeding up the training process. Normalization ensures that the network weights don't have to compensate for vastly different input scales, which can lead to faster convergence and better performance.\n","\n","#### Why Flatten the Images?\n","\n","Flattening the images into one-dimensional vectors is necessary when using fully connected (dense) layers in a neural network. Fully connected layers expect a flat input vector rather than a 2D matrix. By flattening the 28x28 images into 784-dimensional vectors, we can directly feed them into a fully connected neural network. This step is particularly important for traditional multi-layer perceptron models, where the input layer needs to match the dimensionality of the flattened image data."],"metadata":{"id":"Ls_V94Npqhs-"}},{"cell_type":"markdown","source":["## Section 2: Model Design and Training"],"metadata":{"id":"g0fm4MNsMVBX"}},{"cell_type":"markdown","source":["### Step 3: Build and Train the Neural Network\n","\n","In this step, we will build a neural network using PyTorch to classify the MNIST images. The network will consist of at least two hidden layers and an output layer with softmax activation. We will then create an optimizer to update the network's weights during training."],"metadata":{"id":"s-aHnXDSMnDq"}},{"cell_type":"code","source":["# Import torch.nn and torch.optim\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","class NeuralNet(nn.Module):\n","    \"\"\"\n","    A simple neural network with two hidden layers for MNIST classification.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"\n","        Initializes the neural network layers.\n","        - fc1: First fully connected layer (input: 28x28=784, output: 512)\n","        - fc2: Second fully connected layer (input: 512, output: 256)\n","        - fc3: Output fully connected layer (input: 256, output: 10)\n","        \"\"\"\n","        super(NeuralNet, self).__init__()\n","        self.fc1 = nn.Linear(28 * 28, 512)  # First hidden layer\n","        self.fc2 = nn.Linear(512, 256)  # Second hidden layer\n","        self.fc3 = nn.Linear(256, 10)  # Output layer\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Defines the forward pass of the network.\n","\n","        Args:\n","        - x (torch.Tensor): Input tensor (flattened images).\n","\n","        Returns:\n","        - torch.Tensor: Output logits for each class.\n","        \"\"\"\n","        x = torch.relu(self.fc1(x))  # Apply ReLU activation to first hidden layer\n","        x = torch.relu(self.fc2(x))  # Apply ReLU activation to second hidden layer\n","        x = self.fc3(x)  # Output layer (no activation, as we will use CrossEntropyLoss)\n","        return x\n","\n","# Define the loss function (cross-entropy loss for classification)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Initialize the optimizer (Adam) with the model parameters\n","model = NeuralNet()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 5  # Define the number of epochs\n","\n","for epoch in range(num_epochs):\n","    for images, labels in train_loader:\n","        # Zero the gradients to prevent accumulation from previous iterations\n","        optimizer.zero_grad()\n","\n","        # Perform a forward pass through the network\n","        outputs = model(images)\n","\n","        # Compute the loss between the predicted outputs and true labels\n","        loss = criterion(outputs, labels)\n","\n","        # Perform a backward pass to compute gradients\n","        loss.backward()\n","\n","        # Update the model's weights using the optimizer\n","        optimizer.step()\n","\n","    # Print training progress\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"],"metadata":{"id":"wle9cAylMu1M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716458049055,"user_tz":-60,"elapsed":100267,"user":{"displayName":"Paschal Ugwu","userId":"02920440952136523881"}},"outputId":"4704ef39-809b-4e7a-c291-b8f5c083508c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/5], Loss: 0.0574\n","Epoch [2/5], Loss: 0.0488\n","Epoch [3/5], Loss: 0.0277\n","Epoch [4/5], Loss: 0.1244\n","Epoch [5/5], Loss: 0.0003\n"]}]},{"cell_type":"markdown","source":["### Justification for Using Cross-Entropy Loss, Adam Optimizer, and 5 Epochs\n","\n","#### Cross-Entropy Loss\n","\n","**Reason for Use:**\n","Cross-entropy loss, also known as log loss, is the most commonly used loss function for classification problems, including multi-class classification like the MNIST digit classification task. It measures the performance of a classification model whose output is a probability value between 0 and 1.\n","\n","- **Mathematical Definition:** Cross-entropy loss is defined as $L = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{c=1}^C y_{i,c} \\log(p_{i,c})$, where $N$ is the number of samples, $C$ is the number of classes, $y_{i,c}$ is a binary indicator (0 or 1) if class label $c$ is the correct classification for sample $i$, and $p_{i,c}$ is the predicted probability that sample $i$ is of class $c$.\n","\n","- **Why Cross-Entropy Loss?**\n","  - **Probabilistic Interpretation:** It directly measures the difference between the true label distribution and the predicted label distribution.\n","  - **Gradient Properties:** It has well-behaved gradients that facilitate efficient training with gradient descent methods.\n","  - **Interpretability:** The value of cross-entropy loss gives an indication of how well the model is performing.\n","\n","#### Adam Optimizer\n","\n","**Reason for Use:**\n","The Adam (Adaptive Moment Estimation) optimizer is an extension of stochastic gradient descent that has been widely adopted due to its adaptive learning rate and momentum.\n","\n","- **Algorithm:** Adam combines the advantages of two other extensions of stochastic gradient descent, namely Adagrad and RMSProp. It computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.\n","\n","- **Why Adam Optimizer?**\n","  - **Adaptive Learning Rate:** Adjusts the learning rate for each parameter dynamically, which helps in faster convergence.\n","  - **Robust to Sparse Gradients:** Handles sparse gradients on noisy problems effectively.\n","  - **Less Parameter Tuning:** Often works well out-of-the-box with less need to tune the learning rate.\n","  - **Efficiency:** Computationally efficient with low memory requirements.\n","\n","#### 5 Epochs\n","\n","**Reason for Use:**\n","An epoch represents one complete pass through the entire training dataset. The choice of the number of epochs depends on the dataset size, model complexity, and the desired trade-off between training time and performance.\n","\n","- **Why 5 Epochs?**\n","  - **Initial Training:** For the MNIST dataset and a simple neural network, 5 epochs are typically sufficient to achieve reasonable performance without overfitting.\n","  - **Performance Monitoring:** With fewer epochs, we can quickly monitor the training process and adjust hyperparameters if necessary.\n","  - **Avoid Overfitting:** Limiting the number of epochs helps prevent overfitting, especially in an initial training phase where the model architecture and other hyperparameters are being validated.\n","\n","In practice, the number of epochs is usually determined experimentally by monitoring the model's performance on a validation set and using techniques such as early stopping to determine the optimal point to stop training."],"metadata":{"id":"vh5VMbBhvsp-"}},{"cell_type":"markdown","source":["## Section 3: Model Testing and Evaluation"],"metadata":{"id":"g_QJxHF-M4T-"}},{"cell_type":"markdown","source":["### Step 4: Evaluate the Model\n","\n","In this step, we will evaluate the trained neural network's accuracy on the test set. We will then tune the model's hyperparameters and architecture to achieve at least 90% accuracy on the test set."],"metadata":{"id":"FYsGPOSxM88P"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","def evaluate_model(model, test_loader):\n","    \"\"\"\n","    Evaluate the model on the test set using DataLoader.\n","\n","    Args:\n","    - model (nn.Module): Trained neural network model.\n","    - test_loader (DataLoader): DataLoader for the test dataset.\n","\n","    Returns:\n","    - float: Accuracy of the model on the test set.\n","    \"\"\"\n","    model.eval()  # Set the model to evaluation mode\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():  # Disable gradient calculation for evaluation\n","        for images, labels in test_loader:\n","            outputs = model(images)  # Perform a forward pass through the network\n","            _, predicted = torch.max(outputs.data, 1)  # Get the predicted class\n","            total += labels.size(0)  # Update the total count\n","            correct += (predicted == labels).sum().item()  # Update the correct predictions count\n","\n","    accuracy = 100 * correct / total  # Calculate the accuracy\n","    return accuracy\n","\n","# Calculate and print the accuracy on the test set using DataLoader\n","test_accuracy = evaluate_model(model, test_loader)\n","print(f'Test Accuracy: {test_accuracy:.2f}%')\n","\n","class NeuralNet(nn.Module):\n","    \"\"\"\n","    A simple neural network with modified architecture for MNIST classification.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"\n","        Initializes the neural network architecture.\n","\n","        The neural network consists of three fully connected layers:\n","        - First hidden layer with 1024 units.\n","        - Second hidden layer with 512 units.\n","        - Output layer with 10 units for classification.\n","        \"\"\"\n","        super(NeuralNet, self).__init__()\n","        self.fc1 = nn.Linear(28 * 28, 1024)  # First hidden layer with more units\n","        self.fc2 = nn.Linear(1024, 512)  # Second hidden layer\n","        self.fc3 = nn.Linear(512, 10)  # Output layer\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the neural network.\n","\n","        Applies ReLU activation to the first and second hidden layers.\n","        No activation is applied to the output layer as CrossEntropyLoss is used.\n","        \"\"\"\n","        x = torch.relu(self.fc1(x))  # Apply ReLU activation to first hidden layer\n","        x = torch.relu(self.fc2(x))  # Apply ReLU activation to second hidden layer\n","        x = self.fc3(x)  # Output layer\n","        return x\n","\n","# Initialize the modified model\n","model = NeuralNet()\n","\n","# Reinitialize the optimizer with the new model parameters\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Retrain the model (similar to the previous training loop)\n","num_epochs = 5  # Define the number of epochs\n","\n","for epoch in range(num_epochs):\n","    for images, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","# Recalculate the accuracy on the test set using DataLoader\n","test_accuracy = evaluate_model(model, test_loader)\n","print(f'New Test Accuracy: {test_accuracy:.2f}%')"],"metadata":{"id":"vb4cnrmLNBrS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716458204597,"user_tz":-60,"elapsed":155572,"user":{"displayName":"Paschal Ugwu","userId":"02920440952136523881"}},"outputId":"69a9cff9-9bf5-4889-b384-22037303f76b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 97.49%\n","Epoch [1/5], Loss: 0.0107\n","Epoch [2/5], Loss: 0.2411\n","Epoch [3/5], Loss: 0.0414\n","Epoch [4/5], Loss: 0.0053\n","Epoch [5/5], Loss: 0.0071\n","New Test Accuracy: 97.38%\n"]}]},{"cell_type":"markdown","source":["### Hyperparameter Tuning Process and Results\n","\n","**Initial Model:**\n","- Two hidden layers with 512 and 256 units respectively.\n","- Test Accuracy: 97.49%\n","\n","**Modified Model:**\n","- Increased the number of units in the first hidden layer to 1024 and in the second hidden layer to 512.\n","- Retrained the model for 5 epochs.\n","\n","**Results:**\n","- New Test Accuracy: 97.38%\n","\n","**Conclusion:**\n","- Despite increasing the number of units in the hidden layers, the model's accuracy on the test set slightly decreased from 97.49% to 97.38%.\n","- This suggests that simply increasing the model's capacity may not always lead to improved performance.\n"],"metadata":{"id":"7R5cAjXJ1Jb8"}},{"cell_type":"markdown","source":["### Step 5: Save the Trained Model\n","\n","Finally, we will save the trained model using `torch.save` so that it can be reloaded and used later without retraining."],"metadata":{"id":"YEFvZtOHNJSA"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import matplotlib.pyplot as plt\n","import os\n","\n","# Create directory if it does not exist\n","os.makedirs(os.path.dirname('./data/trained_model.pth'), exist_ok=True)\n","\n","# Save the trained model's state dictionary using torch.save\n","def save_model(model, filepath):\n","    \"\"\"\n","    Save the trained model's state dictionary to a file.\n","\n","    Args:\n","    - model (nn.Module): Trained neural network model.\n","    - filepath (str): Path to save the model's state dictionary.\n","    \"\"\"\n","    torch.save(model.state_dict(), filepath)\n","\n","# Specify the file path to save the model\n","model_filepath = './data/trained_model.pth'\n","\n","# Save the trained model\n","save_model(model, model_filepath)\n","print(f\"Trained model saved at '{model_filepath}'\")\n","\n","# Load the model's state dictionary (to verify saving and loading process)\n","def load_model(model_class, filepath):\n","    \"\"\"\n","    Load the model's state dictionary from a file and initialize the model.\n","\n","    Args:\n","    - model_class (class): Class of the neural network model.\n","    - filepath (str): Path to load the model's state dictionary.\n","\n","    Returns:\n","    - nn.Module: Loaded neural network model.\n","    \"\"\"\n","    loaded_model = model_class()  # Initialize model with the same architecture\n","    loaded_model.load_state_dict(torch.load(filepath))  # Load the saved state dictionary\n","    return loaded_model\n","\n","# Ensure the loaded model performs correctly on a test batch\n","def test_loaded_model(loaded_model, test_loader):\n","    \"\"\"\n","    Test the loaded model on the entire test dataset to verify its correctness.\n","\n","    Args:\n","    - loaded_model (nn.Module): Loaded neural network model.\n","    - test_loader (DataLoader): DataLoader for the test dataset.\n","    \"\"\"\n","    loaded_model.eval()  # Set the model to evaluation mode\n","    all_predicted_labels = []\n","    all_true_labels = []\n","\n","    with torch.no_grad():  # Disable gradient calculation\n","        for images, labels in test_loader:\n","            outputs = loaded_model(images)  # Make predictions on the test batch\n","            predicted_labels = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n","            all_predicted_labels.extend(predicted_labels)\n","            all_true_labels.extend(labels.numpy())\n","\n","    # Convert accumulated lists to numpy arrays\n","    all_predicted_labels = np.array(all_predicted_labels)\n","    all_true_labels = np.array(all_true_labels)\n","\n","    # Calculate evaluation metrics\n","    accuracy = accuracy_score(all_true_labels, all_predicted_labels)\n","    precision = precision_score(all_true_labels, all_predicted_labels, average='weighted')\n","    recall = recall_score(all_true_labels, all_predicted_labels, average='weighted')\n","    f1 = f1_score(all_true_labels, all_predicted_labels, average='weighted')\n","\n","    # Print evaluation metrics\n","    print(f'Accuracy: {accuracy:.4f}')\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'F1 Score: {f1:.4f}')\n","\n","# Load the saved model and verify its correctness\n","loaded_model = load_model(NeuralNet, model_filepath)\n","test_loaded_model(loaded_model, test_loader)\n","print(\"Loaded model verified on the entire test dataset\")"],"metadata":{"id":"ffAQD6x5NWJL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716461369094,"user_tz":-60,"elapsed":2756,"user":{"displayName":"Paschal Ugwu","userId":"02920440952136523881"}},"outputId":"6a810b8f-720c-4be4-b365-39361b53391c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Trained model saved at './data/trained_model.pth'\n","Accuracy: 0.9738\n","Precision: 0.9742\n","Recall: 0.9738\n","F1 Score: 0.9738\n","Loaded model verified on the entire test dataset\n"]}]},{"cell_type":"markdown","source":["After successfully training our neural network model, it's essential to save it so that we can use it later without having to retrain from scratch. We accomplish this using the following steps:\n","\n","1. **Creating Directory (if not exist):** First, we ensure that the directory where we intend to save our trained model exists. We use the `os.makedirs` function to create the directory if it doesn't already exist.\n","\n","2. **Saving the Model:** We define a function `save_model` that takes our trained model and a file path as input. Inside this function, we use `torch.save` to save the model's state dictionary to the specified file path. The state dictionary contains all the learnable parameters of the model.\n","\n","3. **Specifying the File Path:** We specify the file path where we want to save our trained model. This file path will be used by the `save_model` function to save the model.\n","\n","4. **Loading the Model (for Verification):** To ensure that our saving and loading process works correctly, we define a function `load_model`. This function takes the class of our neural network model and the file path of the saved model as input. It initializes a new model with the same architecture as the original trained model and loads the saved state dictionary using `torch.load`.\n","\n","5. **Testing the Loaded Model:** Finally, we test the loaded model on the entire test dataset to verify its correctness. We evaluate the model's performance by calculating metrics such as accuracy, precision, recall, and F1 score. This ensures that the loaded model behaves as expected and can be used confidently for inference tasks.\n","\n","By following these steps, we ensure that our trained model is safely stored and can be reused for various purposes such as deployment, further analysis, or sharing with others."],"metadata":{"id":"l6T6KyhRGCvW"}},{"cell_type":"markdown","source":["## Additional Steps to Make Our Project Stand Out\n","\n","To make our project stand out, we'll consider implementing the following suggestions:\n","- Implement a validation set to monitor accuracy during training. This will help us tune the model more effectively and avoid overfitting.\n","- Use a more advanced architecture like a convolutional neural network (CNN) to improve model accuracy. CNNs are specifically designed for image data and can capture spatial hierarchies better than fully connected networks.\n","- Contextualize our model’s performance by comparing it with the results from Yann LeCun’s webpage. This will provide a benchmark and a sense of how well our model performs in a broader context."],"metadata":{"id":"w4U-yTKdNbx9"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","# Step 1: Implementing a validation set\n","\n","# Load the MNIST dataset and apply transformations\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,))\n","])\n","\n","# Load the training dataset\n","train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","\n","# Define the size of the validation set\n","val_size = int(0.2 * len(train_dataset))\n","train_size = len(train_dataset) - val_size\n","\n","# Split the dataset into training and validation sets\n","train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n","\n","# Create DataLoader objects for the training and validation sets\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n","\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n","    \"\"\"\n","    Train a neural network model.\n","\n","    Args:\n","        model (nn.Module): The neural network model to train.\n","        train_loader (DataLoader): DataLoader for the training dataset.\n","        val_loader (DataLoader): DataLoader for the validation dataset.\n","        criterion: The loss function.\n","        optimizer: The optimization algorithm.\n","        num_epochs (int): Number of training epochs (default is 5).\n","    \"\"\"\n","    for epoch in range(num_epochs):\n","        # Training phase\n","        model.train()\n","        for images, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","        # Validation phase\n","        model.eval()\n","        val_correct = 0\n","        val_total = 0\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                outputs = model(images)\n","                _, predicted = torch.max(outputs.data, 1)\n","                val_total += labels.size(0)\n","                val_correct += (predicted == labels).sum().item()\n","        val_accuracy = val_correct / val_total\n","\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {val_accuracy:.4f}')\n","\n","# Step 3: Using a Convolutional Neural Network (CNN)\n","\n","class CNN(nn.Module):\n","    \"\"\"\n","    Convolutional Neural Network (CNN) model for image classification.\n","\n","    This class defines the architecture of the CNN model.\n","    \"\"\"\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the CNN model.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output tensor.\n","        \"\"\"\n","        x = self.pool(torch.relu(self.conv1(x)))\n","        x = self.pool(torch.relu(self.conv2(x)))\n","        x = x.view(-1, 64 * 7 * 7)\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","# Step 4: Comparing with Yann LeCun’s results\n","\n","# Retrieve benchmark accuracy from Yann LeCun’s webpage\n","benchmark_accuracy = 0.99  # Placeholder value, replace with actual benchmark accuracy\n","\n","criterion = nn.CrossEntropyLoss()\n","model = CNN()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","train_model(model, train_loader, val_loader, criterion, optimizer)\n","\n","# Evaluate the model on the test set to get the final accuracy\n","test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","model.eval()\n","test_correct = 0\n","test_total = 0\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        test_total += labels.size(0)\n","        test_correct += (predicted == labels).sum().item()\n","test_accuracy = test_correct / test_total\n","\n","print(f'Test Accuracy: {test_accuracy:.4f}')\n","print(f'Benchmark Accuracy: {benchmark_accuracy:.4f}')\n","print(f'Our Model vs. Benchmark: {test_accuracy - benchmark_accuracy:.4f}')"],"metadata":{"id":"QHgYCv_5N-UV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716462623191,"user_tz":-60,"elapsed":320707,"user":{"displayName":"Paschal Ugwu","userId":"02920440952136523881"}},"outputId":"869ee95f-8200-4b44-aae0-c10b303d2c97"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/5], Validation Accuracy: 0.9832\n","Epoch [2/5], Validation Accuracy: 0.9878\n","Epoch [3/5], Validation Accuracy: 0.9868\n","Epoch [4/5], Validation Accuracy: 0.9908\n","Epoch [5/5], Validation Accuracy: 0.9898\n","Test Accuracy: 0.9906\n","Benchmark Accuracy: 0.9900\n","Our Model vs. Benchmark: 0.0006\n"]}]},{"cell_type":"markdown","source":["1. **Implement a Validation Set:**\n","   To enhance the robustness of our model and facilitate more effective hyperparameter tuning, it's advisable to implement a validation set. This set can be used to monitor the model's performance during training, helping us make decisions regarding the learning rate, model architecture, and other hyperparameters. By splitting the training data into training and validation sets, we can assess the model's generalization ability and prevent overfitting.\n","\n","2. **Explore Advanced Architectures like CNNs:**\n","   While our current model achieves commendable accuracy, exploring more advanced architectures like Convolutional Neural Networks (CNNs) could further improve performance, especially for image recognition tasks like MNIST digit classification. CNNs are specifically designed to capture spatial hierarchies in images, making them more adept at extracting meaningful features from visual data. Implementing a CNN architecture could potentially lead to higher accuracy and better generalization.\n","\n","3. **Contextualize Model Performance with Benchmark Comparisons:**\n","   To provide a comprehensive assessment of our model's performance, it's beneficial to compare its results with established benchmarks. Yann LeCun's webpage often serves as a reference for benchmark MNIST results. By comparing our model's accuracy with these benchmarks, we gain insights into how well our model performs relative to state-of-the-art approaches. This contextualization helps validate our model's efficacy and provides valuable information for further improvements.\n","\n","By incorporating these additional steps, we can elevate the quality and credibility of our project, ensuring that our character recognition system is both robust and competitive within the field of machine learning."],"metadata":{"id":"RNuTsIRSJ-gF"}},{"cell_type":"markdown","source":["# Final Note:\n","\n","In this project, we meticulously curated the MNIST dataset, preprocessing it to align with PyTorch's requirements. Through careful visualization, we confirmed the integrity of our data. Building a neural network with two hidden layers, we optimized its performance with Cross-Entropy Loss and Adam Optimizer over 5 epochs. Our model achieved commendable accuracy, which we further refined by modifying its architecture and retraining. By saving our trained model, we ensured its accessibility for future tasks. To stand out, we proposed implementing a validation set, exploring advanced architectures like CNNs, and contextualizing our model's performance against established benchmarks. Through these steps, we fortified our character recognition system, making it robust and competitive in the realm of machine learning."],"metadata":{"id":"nJAj3qCoLMlK"}}]}